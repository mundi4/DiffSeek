const MANUAL_ANCHOR1 = "@@@";
const MANUAL_ANCHOR2 = "###";

const LINE_START = 1 << 0; // 1
const LINE_END = 1 << 1; // 2
const CONTAINER_START = 1 << 2; // 4
const CONTAINER_END = 1 << 3; // 8
const NO_JOIN = 1 << 4; // @@@, ### ë“±ë“± // 16
const WILD_CARD = 1 << 5;
const MANUAL_ANCHOR = 1 << 6; // 32. @@@, ### ë“±ë“±
const IMAGE = 1 << 7;

const SECTION_HEADING_BIT = 10;
const SECTION_HEADING_TYPE1 = 1 << (SECTION_HEADING_BIT + 0); // 1.
const SECTION_HEADING_TYPE2 = 1 << (SECTION_HEADING_BIT + 1); // ê°€.
const SECTION_HEADING_TYPE3 = 1 << (SECTION_HEADING_BIT + 2); // (1)
const SECTION_HEADING_TYPE4 = 1 << (SECTION_HEADING_BIT + 3); // (ê°€)
const SECTION_HEADING_TYPE5 = 1 << (SECTION_HEADING_BIT + 4); // 1)
const SECTION_HEADING_TYPE6 = 1 << (SECTION_HEADING_BIT + 5); // ê°€)

const LINE_BOUNDARY = LINE_START | LINE_END;
const CONTAINER_BOUNDARY = CONTAINER_START | CONTAINER_END;
const SECTION_HEADING_MASK =
	SECTION_HEADING_TYPE1 | SECTION_HEADING_TYPE2 | SECTION_HEADING_TYPE3 | SECTION_HEADING_TYPE4 | SECTION_HEADING_TYPE5 | SECTION_HEADING_TYPE6;

// const normalizeChars: { [ch: string]: string } = {};

// text flow containers?
const containerElements = {
	DIV: true,
	PRE: true,
	BLOCKQUOTE: true,
	LI: true,
	TD: true,
	TH: true,
	SECTION: true,
	ARTICLE: true,
	HEADER: true,
	FOOTER: true,
	ASIDE: true,
	MAIN: true,
	CAPTION: true,
	FIGURE: true,
	FIGCAPTION: true,
};

const spaceChars: Record<string, boolean> = {
	" ": true,
	"\t": true,
	"\n": true,
	"\r": true, // ê¸€ì„...
	"\f": true, // ì´ê²ƒë“¤ì€...
	"\v": true, // ë³¼ì¼ì´ ì—†ì„ê²ƒ...
	"\u00A0": true, // &nbsp; ??
};

const splitChars: Record<string, boolean> = {
	"(": true,
	")": true,
	"[": true,
	"]": true,
	"{": true,
	"}": true,
};

const normalizedCharMap = ((normChars: (string | number)[][]) => {
	const result: Record<string, string> = {};
	let parser: DOMParser;
	function htmlEntityToChar(entity: string) {
		const doc = (parser = parser || new DOMParser()).parseFromString(entity, "text/html");
		const char = doc.body.textContent!;
		if (char.length !== 1) {
			throw new Error("htmlEntityToChar: not a single character entity: " + entity);
		}
		return char;
	}

	for (const entry of normChars) {
		const [norm, ...variants] = entry;
		for (const variant of variants) {
			if (typeof variant === "number") {
				result[String.fromCharCode(variant)] = norm as string;
			} else if (typeof variant === "string") {
				if (variant.length === 1 || (variant.length === 2 && variant.charCodeAt(0) >= 0xd800)) {
					result[variant] = norm as string;
				} else if (variant[0] === "&") {
					result[htmlEntityToChar(variant)] = norm as string;
				}
			}
		}
	}
	return result;
})([
	['"', "â€œ", "â€", "'", "â€˜", "â€™"], // ë¹„ì¦ˆí”Œë«í¼ í¸ì§‘ê¸°ì—ì„œ ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ ë°”ê¾¸ì–´ë²„ë¦¼. WHY?
	["-", "â€", "â€‘", "â€’", "â€“", "ï¹˜", "â€”", "ï¼"],
	[".", "â€¤", "ï¼"],
	[",", "ï¼Œ"],
	["â€¢", "â—"], // ì´ê±¸ ì¤‘ê°„ì  ìš©ë„ë¡œ ì“°ëŠ” ì‚¬ëŒë“¤ì€ ì •ë§ ê°ˆì•„ë§ˆì…”ì•¼ëœë‹¤. ë„ì €íˆ ìš©ë‚©í•´ì¤„ ìˆ˜ ì—†ê³  ê°™ì€ ë¬¸ìë¡œ ì¸ì‹í•˜ê²Œ ë§Œë“¤ê³  ì‹¶ì§€ ì•Šë‹¤.
	["â—¦", "â—‹", "ã…‡"], // ììŒ "ì´ì‘"ì„ ì“°ëŠ” ì‚¬ëŒë“¤ë„ ê°œì¸ì ìœ¼ë¡œ ì´í•´ê°€ ì•ˆë˜ì§€ë§Œ ë§ë”ë¼.
	["â– ", "â–ª", "â—¼"],
	["â–¡", "â–«", "â—»", "ã…"],
	["Â·", "â‹…", "âˆ™", "ã†", "â€§"], // ìœ ë‹ˆì½”ë“œë¥¼ ë§Œë“  ì§‘ë‹¨ì€ ë„ëŒ€ì²´ ë¬´ìŠ¨ ìƒê°ì´ì—ˆë˜ê±¸ê¹Œ?...
	["â€¦", "â‹¯"],
	["(", "ï¼ˆ"],
	[")", "ï¼‰"],
	["[", "ï¼»"],
	["]", "ï¼½"],
	["{", "ï½›"],
	["}", "ï½"],
	["<", "ï¼œ"],
	[">", "ï¼"],
	["=", "ï¼"],
	["+", "ï¼‹"],
	["*", "ï¼Š", "âœ±", "Ã—", "âˆ—"],
	["/", "ï¼", "Ã·"],
	["\\", "â‚©"], // ì•„ë§ˆë„ ì›í™” ê¸°í˜¸ë¡œ ì‚¬ìš©í–ˆê² ì§€
	["&", "ï¼†"],
	["#", "ï¼ƒ"],
	["@", "ï¼ "],
	["$", "ï¼„"],
	["%", "ï¼…"],
	["^", "ï¼¾"],
	["~", "ï½"],
	["`", "ï½€"],
	["|", "ï½œ"],
	[":", "ï¼š"],
	[";", "ï¼›"],
	["?", "ï¼Ÿ"],
	["!", "ï¼"],
	["_", "ï¼¿"],
	["â†’", "â‡’", "â¡", "â”", "â", "âŸ"],
	["â†", "â‡", "â¬…", "âŸµ", "âŸ¸"],
	["â†‘", "â‡‘", "â¬†"],
	["â†“", "â‡“", "â¬‡"],
	["â†”", "â‡”"],
	["â†•", "â‡•"],
]);

const TOKEN_CACHE_SIZE = 2;

type TokenCacheEntry = {
	text: string;
	tokens: Token[];
};

const tokenCache: Record<TokenizationMode, TokenCacheEntry[]> = {
	["char"]: [],
	["word"]: [],
	["line"]: [],
};

// wildcards.
// ì´ê±¸ ì–´ë–»ê²Œ êµ¬í˜„í•´ì•¼í• ì§€ ê°ì´ ì•ˆì˜¤ì§€ë§Œ ì§€ê¸ˆìœ¼ë¡œì¨ëŠ” ì–˜ë„¤ë“¤ì„ atomicí•˜ê²Œ ì·¨ê¸‰(ì‚¬ì´ì— ê³µë°±ì´ ìˆì–´ë„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë§Œë“¬. '(í˜„í–‰ê³¼ ê°™ìŒ)'ì—ì„œ ì¼ë¶€ë¶„ë§Œ ë§¤ì¹˜ë˜ëŠ” ê²ƒì„ ë°©ì§€)
// ê¸€ìë‹¨ìœ„ë¡œ í† í°í™”í•˜ëŠ” ê²½ìš°ì—ë„ ì–˜ë„¤ë“¤ì€ (...) í†µì±„ë¡œ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰.
// ì™€ì¼ë“œì¹´ë“œdiffì¸ ê²½ìš° ë‹¤ë¥¸ diffì™€ ë³‘í•©ë˜ì§€ ì•Šìœ¼ë©´ ì¢‹ì§€ë§Œ ì™€ì¼ë“œì¹´ë“œê°€ ì–¼ë§ˆë‚˜ greedyí•˜ê²Œ ë°˜ëŒ€ìª½ í…ìŠ¤íŠ¸ë¥¼ ì¡ì•„ë¨¹ì–´ì•¼ í• ì§€
// ì–‘ìª½ì— wildcardê°€ ë™ì‹œì— ë‚˜ì˜¤ëŠ” ê²½ìš° ê²½ê³„ë¥¼ ì–´ë””ì„œ ì–´ë–»ê²Œ ì§¤ë¼ì•¼í• ì§€ ì‰½ì§€ ì•ŠìŒ.
// ë˜í•œ wildcardë¥¼ ê°•ì œë¡œ ë‹¤ë¥¸ diffì™€ ë¶„ë¦¬í•˜ëŠ” ê²½ìš° diffê°€ ê°™ì€ ìœ„ì¹˜ì— ë‘ ê°œ ì´ìƒ ìƒê¸°ê²Œ ë˜ëŠ” ìˆ˜ê°€ ìˆë‹¤. (wildcardì™€ wildcardê°€ ì•„ë‹Œ ê²ƒ)
// ì´ ê²½ìš° ì •í™•íˆ ê°™ì€ ìœ„ì¹˜ì— ë‘ê°œì˜ diffë¥¼ ë Œë”ë§í•´ì•¼í•˜ê³  ê²°êµ­ ë‘ê°œê°€ ê²¹ì³ë³´ì´ê²Œ ë˜ëŠ”ë° ë¶„ê°„ì´ ì˜ ì•ˆëœë‹¤.
const wildcardTrie = createTrie(true);
wildcardTrie.insert("(ì¶”ê°€)", WILD_CARD);
wildcardTrie.insert("(ì‚­ì œ)", WILD_CARD);
wildcardTrie.insert("(ì‹ ì„¤)", WILD_CARD);
wildcardTrie.insert("(ìƒëµ)", WILD_CARD);
wildcardTrie.insert("(í˜„í–‰ê³¼ê°™ìŒ)", WILD_CARD);

const wildcardTrieNode = wildcardTrie.root.next("(")!;

const sectionHeadingTrie = createTrie(false);
for (let i = 1; i < 40; i++) {
	sectionHeadingTrie.insert(`${i}.`, SECTION_HEADING_TYPE1);
	sectionHeadingTrie.insert(`(${i})`, SECTION_HEADING_TYPE3);
	sectionHeadingTrie.insert(`${i})`, SECTION_HEADING_TYPE5);
}

for (let i = 0; i < HANGUL_ORDER.length; i++) {
	sectionHeadingTrie.insert(`${HANGUL_ORDER[i]}.`, SECTION_HEADING_TYPE2);
	sectionHeadingTrie.insert(`(${HANGUL_ORDER[i]})`, SECTION_HEADING_TYPE4);
	sectionHeadingTrie.insert(`${HANGUL_ORDER[i]})`, SECTION_HEADING_TYPE6);
}
const SectionHeadingTrieNode = sectionHeadingTrie.root;
const sectionHeadingStartChars = extractStartCharsFromTrie(SectionHeadingTrieNode);

const manualAnchorTrie = createTrie(false);
manualAnchorTrie.insert(MANUAL_ANCHOR1, MANUAL_ANCHOR);
manualAnchorTrie.insert(MANUAL_ANCHOR2, MANUAL_ANCHOR);
const manualAnchorTrieNode = manualAnchorTrie.root;
const manualAnchorStartChars = extractStartCharsFromTrie(manualAnchorTrieNode);

function tokenizeByChar(input: string): Token[] {
	const tokens: Token[] = [];
	let lineNum = 1;
	let flags = LINE_START;
	const inputEnd = input.length;

	for (let i = 0; i < inputEnd; i++) {
		const ch = input[i];

		if (!spaceChars[ch]) {
			if (ch === "(") {
				const result = findInTrie(wildcardTrieNode, input, i + 1);
				if (result) {
					if (tokens.length === 0 || checkIfFirstOfLine(input, i)) {
						flags |= LINE_START;
					}
					tokens.push({
						text: result.word,
						pos: i,
						len: result.end - i,
						lineNum,
						flags: flags | result.flags,
					});
					flags = 0;
					i = result.end - 1;
					continue;
				}
			}

			if (manualAnchorStartChars[ch]) {
				const nextNode = manualAnchorTrieNode.next(ch)!;
				const result = findInTrie(nextNode, input, i + 1);
				if (result) {
					if (tokens.length === 0 || checkIfFirstOfLine(input, i)) {
						flags |= LINE_START;
					}
					tokens.push({
						text: result.word,
						pos: i,
						len: result.end - i,
						lineNum,
						flags: flags | result.flags,
					});
					flags = 0;
					i = result.end - 1;
					continue;
				}
			}

			if (tokens.length === 0 || checkIfFirstOfLine(input, i)) {
				flags |= LINE_START;
			}
			const normalized = normalizedCharMap[ch] || ch;
			tokens.push({
				text: normalized,
				pos: i,
				len: 1,
				lineNum,
				flags,
			});
			flags = 0;
		}

		if (ch === "\n") {
			lineNum++;
			flags = LINE_START;
			if (tokens.length) {
				tokens[tokens.length - 1].flags |= LINE_END;
			}
		}
	}

	if (tokens.length) {
		tokens[tokens.length - 1].flags |= LINE_END;
	}

	return tokens;
}

function tokenizeByWord(input: string): Token[] {
	const tokens: Token[] = [];
	let currentStart = -1;
	let lineNum = 1;
	let flags = LINE_START;
	let shouldNormalize = false;
	const inputEnd = input.length;

	function emitToken(end: number) {
		const raw = input.slice(currentStart, end);
		const normalized = shouldNormalize ? normalize(raw) : raw;

		flags |= tokens.length === 0 || checkIfFirstOfLine(input, currentStart) ? LINE_START : 0;
		if (normalized === MANUAL_ANCHOR1 || normalized === MANUAL_ANCHOR2) {
			flags |= MANUAL_ANCHOR;
		}

		tokens.push({
			text: normalized,
			pos: currentStart,
			len: end - currentStart,
			lineNum,
			flags,
		});

		currentStart = -1;
		flags = 0;
		shouldNormalize = false;
	}

	for (let i = 0; i < inputEnd; i++) {
		let ch = input[i];

		if (spaceChars[ch]) {
			if (currentStart !== -1) emitToken(i);
			if (ch === "\n") {
				lineNum++;
				flags = LINE_START;
				if (tokens.length) tokens[tokens.length - 1].flags |= LINE_END;
			}
			continue;
		}

		if (ch === "(") {
			const result = findInTrie(wildcardTrieNode, input, i);
			if (result) {
				if (currentStart !== -1) emitToken(i);
				flags |= tokens.length === 0 || checkIfFirstOfLine(input, i) ? LINE_START : 0;

				tokens.push({
					text: result.word,
					pos: i,
					len: result.end - i,
					lineNum,
					flags: flags | result.flags,
				});
				flags = 0;
				currentStart = -1;
				i = result.end - 1;
				continue;
			}
		}

		if (currentStart === -1 && flags & LINE_START && sectionHeadingStartChars[ch]) {
			const result = findInTrie(SectionHeadingTrieNode, input, i);
			if (result) {
				const nextChar = input[result.end];
				if (nextChar === " " || nextChar === "\t" || nextChar === "\u00A0") {
					flags |= result.flags;
				}

				// let p = result.end;
				// while (p < inputEnd && SPACE_CHARS[input[p]]) p++;
				// if (p < inputEnd) flags |= result.flags;
			}
		}

		// if (SPLIT_CHARS[ch]) {
		// 	if (currentStart !== -1) emitToken(i);

		// 	flags |= tokens.length === 0 || checkIfFirstOfLine(input, i) ? FIRST_OF_LINE : 0;

		// 	tokens.push({
		// 		text: ch,
		// 		pos: i,
		// 		len: 1,
		// 		lineNum,
		// 		flags,
		// 	});

		// 	flags = 0;
		// 	currentStart = -1;
		// 	continue;
		// }

		if (normalizedCharMap[ch]) {
			shouldNormalize = true;
		}

		if (currentStart === -1) currentStart = i;
	}

	if (currentStart !== -1) emitToken(inputEnd);

	if (tokens.length) {
		tokens[tokens.length - 1].flags |= LINE_END;
	}

	return tokens;
}

function tokenizeByLine(input: string): Token[] {
	const tokens: Token[] = [];
	let lineNum = 1;
	let flags = LINE_START | LINE_END;
	const inputEnd = input.length;

	let buffer = "";
	let started = false;
	let inSpace = false;
	let pos = -1;

	for (let i = 0; i < inputEnd; i++) {
		const ch = input[i];

		if (ch !== "\n") {
			if (!spaceChars[ch]) {
				if (!started) {
					pos = i;
					started = true;

					const result = findInTrie(SectionHeadingTrieNode, input, i);
					if (result) {
						let p = result.end;
						while (p < inputEnd && spaceChars[input[p]]) p++;
						if (p < inputEnd) flags |= result.flags;
					}
				}
				if (inSpace && buffer.length > 0) buffer += " ";
				buffer += ch;
				inSpace = false;
			} else {
				inSpace = started;
			}
		} else {
			if (started) {
				if (buffer === MANUAL_ANCHOR1 || buffer === MANUAL_ANCHOR2) {
					flags |= MANUAL_ANCHOR;
				}
				tokens.push({
					text: buffer,
					pos,
					len: i - pos,
					lineNum,
					flags,
				});
				buffer = "";
				started = false;
				inSpace = false;
				flags = LINE_START | LINE_END;
			}
			lineNum++;
		}
	}

	return tokens;
}

function tokenize(input: string, mode: TokenizationMode, noCache: boolean = false): Token[] {
	let cacheArr = !noCache && tokenCache[mode];
	if (cacheArr) {
		for (let i = 0; i < cacheArr.length; i++) {
			const cache = cacheArr[i];
			if (cache.text.length === input.length && cache.text === input) {
				if (i !== cacheArr.length - 1) {
					cacheArr.splice(i, 1);
					cacheArr.push(cache);
				}
				return cache.tokens;
			}
		}
	}

	const now = performance.now();
	let tokens: Token[];
	switch (mode) {
		case "char":
			tokens = tokenizeByChar(input);
			break;
		case "word":
			tokens = tokenizeByWord(input);
			break;
		case "line":
			tokens = tokenizeByLine(input);
			break;
		default:
			throw new Error("Unknown tokenization mode: " + mode);
	}
	console.debug("tokenize took %d ms", performance.now() - now);
	// tokens.push({
	// 	text: "",
	// 	pos: input.length,
	// 	len: 0,
	// 	lineNum: tokens.length > 0 ? tokens[tokens.length - 1].lineNum : 1,
	// 	flags: FIRST_OF_LINE | LAST_OF_LINE,
	// });

	if (cacheArr) {
		if (cacheArr.length >= TOKEN_CACHE_SIZE) {
			cacheArr.shift();
		}
		cacheArr.push({ text: input, tokens: tokens as Token[] });
	}

	return tokens;
}

function normalize(text: string) {
	let result = "";
	for (let i = 0; i < text.length; i++) {
		const char = text[i];
		result += normalizedCharMap[char] || char;
	}
	return result;
}

function checkIfFirstOfLine(input: string, pos: number) {
	pos--;
	while (pos >= 0) {
		if (input[pos] === "\n") {
			break;
		} else if (!spaceChars[input[pos]]) {
			return false;
		}
		pos--;
	}
	return true;
}

type TrieNode = {
	next: (char: string) => TrieNode | null;
	addChild: (char: string) => TrieNode;
	word: string | null;
	flags: number;
	children: Record<string, TrieNode>;
};

function createTrie(ignoreSpaces = false) {
	const root = createTrieNode(ignoreSpaces);

	function insert(word: string, flags = 0) {
		let node = root;
		for (let i = 0; i < word.length; i++) {
			node = node.addChild(word[i]);
		}
		node.word = word;
		node.flags = flags;
	}

	return { insert, root };
}

function createTrieNode(ignoreSpaces: boolean): TrieNode {
	const children: Record<string, TrieNode> = {};

	const node: TrieNode = {
		children,
		word: null,
		flags: 0,
		next(char: string) {
			if (ignoreSpaces && char === " ") return node;
			return children[char] || null;
		},
		addChild(char: string) {
			return children[char] ?? (children[char] = createTrieNode(ignoreSpaces));
		},
	};

	return node;
}

function findInTrie(trie: TrieNode, input: string, start: number) {
	let node: TrieNode | null = trie;
	let i = start;
	while (i < input.length) {
		const ch = input[i++];
		node = node!.next(ch);
		if (!node) break;
		if (node.word) {
			return { word: node.word, flags: node.flags, end: i };
		}
	}
	return null;
}

function extractStartCharsFromTrie(trie: TrieNode): Record<string, 1> {
	const table: Record<string, 1> = {};
	for (const ch in trie.children) {
		table[ch] = 1;
	}
	return table;
}

type ContainerInfo = {
	textFlow: boolean;
	tokens: Token[];
};

function tokenizeNode(node: Node): Token[] {
	const startTime = performance.now();
	let textPos = 0;
	let currentToken: Token | null = null;
	const results: Token[] = [];
	function processToken(text: string, start: number, length: number) {
		if (currentToken) {
			currentToken.text += text;
			currentToken.len = textPos - currentToken.pos;
		} else {
			currentToken = {
				text,
				pos: start,
				len: length,
				flags: 0,
				lineNum: 0,
			};
		}
	}

	function finalizeToken(flags: number = 0) {
		if (currentToken) {
			currentToken.len = textPos - currentToken.pos;
			currentToken.flags |= flags;
			results.push(currentToken);
			currentToken = null;
			return 1;
		}
		return 0;
	}

	function traverse(node: Node) {
		if (node.nodeType === 3) {
			const text = node.nodeValue!;
			if (text.length === 0) return;
			let nodeStart = textPos;
			let currentStart = -1;
			for (let i = 0; i < text.length; i++, textPos++) {
				const char = text[i];
				if (spaceChars[char]) {
					if (currentStart >= 0) {
						processToken(text.slice(currentStart, i), nodeStart + currentStart, i - currentStart);
						currentStart = -1;
					}
					finalizeToken();
				} else {
					if (currentStart < 0) {
						currentStart = i;
					}
				}
			}

			if (currentStart >= 0) {
				processToken(text.slice(currentStart), nodeStart + currentStart, text.length - currentStart);
			}
		} else if (node.nodeType === 1) {
			if (node.nodeName === "BR") {
				finalizeToken(LINE_END);
				return;
			}

			if ((node as HTMLElement).className === "img") {
				finalizeToken();
				results.push({
					text: (node as HTMLElement).dataset.src || (node as HTMLImageElement).src || "ğŸ–¼ï¸",
					pos: textPos,
					len: node.textContent!.length,
					lineNum: 0,
					flags: IMAGE | NO_JOIN,
				});
				textPos += node.textContent!.length;
				return;
			}

			(node as HTMLElement).dataset.startOffset = String(textPos);

			if (TEXT_FLOW_CONTAINERS[node.nodeName]) {
				finalizeToken(CONTAINER_END | LINE_END);
			}

			const isTextFlowContainer = TEXT_FLOW_CONTAINERS[node.nodeName];
			const numTokensBefore = results.length;

			for (const child of node.childNodes) {
				traverse(child);
			}

			if (BLOCK_ELEMENTS[node.nodeName]) {
				finalizeToken();
			}

			const firstToken = results[numTokensBefore];
			const lastToken = results[results.length - 1];
			if (isTextFlowContainer) {
				if (firstToken) {
					firstToken.flags |= CONTAINER_START | LINE_START;
				}
				if (lastToken) {
					lastToken.flags |= CONTAINER_END | LINE_END;
				}
			} else if (node.nodeName === "P") {
				if (firstToken) {
					firstToken.flags |= LINE_START;
				}
				if (lastToken) {
					lastToken.flags |= LINE_END;
				}
			}

			(node as HTMLElement).dataset.endOffset = String(textPos);


			// currentContainer = containerStack.pop()!;
		}
	}

	traverse(node);
	finalizeToken();
	const endTime = performance.now();
	console.log("tokenizeNode", node.nodeName, node.nodeValue, results, Math.ceil(endTime - startTime) + "ms");

	return results;
}
